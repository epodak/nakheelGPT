{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "# from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from query_data import _template, CONDENSE_QUESTION_PROMPT, QA_PROMPT, get_chain\n",
    "import pickle\n",
    "import os\n",
    "persist_directory = 'db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$2b$12$e0VZfDCO1C3WpW93xJ0Y4ur064RPEUbKOCOFJSdHrZyEjRqIgW29i']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit_authenticator as stauth\n",
    "stauth.Hasher(['abc']).generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$2b$12$VEyyIaZNws2Pfk2pAxfRJe23YQm6OD6.wVgvUCJFDQs8JyjpLWa1i\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import streamlit_authenticator as stauth\n",
    "# def encrypt_password(config):\n",
    "#     for pass in config[usernames][password]:\n",
    "#         print(pass)\n",
    "\n",
    "\n",
    "with open('config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=SafeLoader)\n",
    "    for user in config['credentials']['usernames']:\n",
    "        key = config['credentials']['usernames'][user]['password']\n",
    "        if key.startswith(\"$\") == False:\n",
    "            enc_key = stauth.Hasher([key]).generate()\n",
    "            print(enc_key[0])\n",
    "            config['credentials']['usernames'][user]['password'] = enc_key[0]\n",
    "\n",
    "with open('config.yaml', 'w') as file:\n",
    "    file.write(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"OPENAI_API_ORGANIZATION\"] = \"org-lwaUBVlPJVS50wZwghHFihUA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "test = os.getenv('ZDOTDIR')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/google_reviews.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    space = f.read().encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "\n",
    "with open(\"data/google_reviews.txt\", \"w\") as f:\n",
    "    space = f.write(space)\n",
    "\n",
    "with open(\"data/nakheel.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    space = f.read().encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "\n",
    "with open(\"data/nakheel.txt\", \"w\") as f:\n",
    "    space = f.write(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = UnstructuredFileLoader(\"state_of_the_union.txt\")\n",
    "def embed_doc():\n",
    "    #check data folder is not empty\n",
    "    if len(os.listdir(\"data\")) > 0:\n",
    "        loader = DirectoryLoader('data', glob=\"**/*.*\", loader_cls=TextLoader)\n",
    "        raw_documents = loader.load()\n",
    "        print(len(raw_documents))\n",
    "        # Split text\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            # Set a really small chunk size, just to show.\n",
    "            chunk_size = 1000,\n",
    "            chunk_overlap  = 0,\n",
    "            length_function = len,\n",
    "        )\n",
    "        print(\"111\")\n",
    "        documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "\n",
    "        # Load Data to vectorstore\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        print(\"222\")\n",
    "        vectorstore = Chroma.from_documents(documents, embeddings, persist_directory=persist_directory)\n",
    "        vectorstore.persist()\n",
    "        vectorstore = None\n",
    "        print(\"333\")\n",
    "\n",
    "\n",
    "        # Save vectorstore\n",
    "        # check if vectorstore.pkl exists\n",
    "        with open(\"vectorstore.pkl\", \"wb\") as f:\n",
    "            pickle.dump(vectorstore, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "111\n",
      "222\n",
      "Running Chroma using direct local API.\n",
      "loaded in 2515 embeddings\n",
      "loaded in 1 collections\n",
      "Persisting DB to disk, putting it in the save folder db\n",
      "PersistentDuckDB del, about to run persist\n",
      "Persisting DB to disk, putting it in the save folder db\n",
      "333\n"
     ]
    }
   ],
   "source": [
    "embed_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "loaded in 5005 embeddings\n",
      "loaded in 1 collections\n",
      "Loading vectorstore...\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma(persist_directory=\"db/\", embedding_function=OpenAIEmbeddings())        \n",
    "print(\"Loading vectorstore...\")\n",
    "chain = get_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(user_input):\n",
    "    docs = vectorstore.similarity_search(user_input, k=20)\n",
    "\n",
    "    print(len(docs))\n",
    "    # PART 2 ADDED: CALLBACK FOR TOKEN USAGE\n",
    "    with get_openai_callback() as cb:\n",
    "        output = chain.run(input=user_input, vectorstore = vectorstore, context=docs, chat_history = [], question= user_input, QA_PROMPT=QA_PROMPT, CONDENSE_QUESTION_PROMPT=CONDENSE_QUESTION_PROMPT, template=_template)\n",
    "        print(cb.total_tokens)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "976\n",
      "The malls under Nakheel are Ibn Battuta Mall, Dragon Mart, Nakheel Mall, Golden Mile Galleria, The Pointe, Club Vista Mare, Souk Marfa, Souk Warsan, The View at The Palm as well as six community retail centres, known as Nakheel Pavilions. Nakheel Mall, located in the heart of Palm Jumeirah, offers over 300 shops, restaurants, entertainment outlets and services, including a 15-screen cinema complex and other attractions across five floors. Nakheel Malls owns and operates a diverse range of world-class retail experiences with a distinct portfolio of iconic destinations, including large-scale shopping centres and retail Pavilions, across Dubai.\n",
      "\n",
      "#['Nakheel Mall', 'Ibn Battuta Mall', 'Dragon Mart']\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\"What are the malls under Nakheel?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if vectorstore.pkl exists\n",
    "if  os.path.exists(\"vectorstore.pkl\"):\n",
    "    with open(\"vectorstore.pkl\", 'rb') as f:\n",
    "        docsearch = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='History' lookup_str='' metadata={'source': 'data/nakheel.txt'} lookup_index=0\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter your query: \")\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ChatVectorDBChain\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "template = \"\"\"You are an AI assistant for answering questions about the Document you have uploaded.\n",
    "You are given the following extracted parts of a long document and a question. Provide a conversational answer. at the end of your answer, add a newline and return a python list of up to three wikipedia topics which are related to the context and question leading with a \"#\" like this wihout mentioning anything else:\n",
    "#['topic1', 'topic2', 'topic3']\n",
    "\n",
    "If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
    "\n",
    "Question: {question}\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "Answer in Markdown:\"\"\"\n",
    "QA_PROMPT = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    "\n",
    "\n",
    "def get_chain(vectorstore):\n",
    "    llm = OpenAI(temperature=0) #gpt-3.5-turbo\n",
    "    qa_chain = ChatVectorDBChain.from_llm(\n",
    "        llm,\n",
    "        vectorstore,\n",
    "        qa_prompt=QA_PROMPT,\n",
    "        condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    \n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "loaded in 2515 embeddings\n",
      "loaded in 1 collections\n",
      "Exiting: Cleaning up .chroma directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/python3-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Exception ignored in: <function PersistentDuckDB.__del__ at 0x7f7ad808d5a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/python3-env/lib/python3.10/site-packages/chromadb/db/duckdb.py\", line 407, in __del__\n",
      "    self.persist()\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/python3-env/lib/python3.10/site-packages/chromadb/db/duckdb.py\", line 361, in persist\n",
      "    self._conn.execute(\n",
      "duckdb.IOException: IO Error: Cannot open file \"/db/chroma-embeddings.parquet\": No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PersistentDuckDB del, about to run persist\n",
      "Persisting DB to disk, putting it in the save folder /db\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma(persist_directory=\"db/\", embedding_function=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f98d6545a8c8823af2d31d4f3495c64f9ce8c884c13cd035e8ae9649e78e139e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
