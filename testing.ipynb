{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader, DirectoryLoader, TextLoader\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"OPENAI_API_ORGANIZATION\"] = \"org-lwaUBVlPJVS50wZwghHFihUA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = UnstructuredFileLoader(\"state_of_the_union.txt\")\n",
    "def embed_doc():\n",
    "    #check data folder is not empty\n",
    "    if len(os.listdir(\"data\")) > 0:\n",
    "        loader = DirectoryLoader('data', glob=\"**/*.*\", loader_cls=TextLoader)\n",
    "        raw_documents = loader.load()\n",
    "        print(len(raw_documents))\n",
    "        # Split text\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            # Set a really small chunk size, just to show.\n",
    "            chunk_size = 1000,\n",
    "            chunk_overlap  = 0,\n",
    "            length_function = len,\n",
    "        )\n",
    "        print(\"111\")\n",
    "        documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "\n",
    "        # Load Data to vectorstore\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        print(\"222\")\n",
    "        vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "        print(\"333\")\n",
    "\n",
    "\n",
    "        # Save vectorstore\n",
    "        # check if vectorstore.pkl exists\n",
    "        with open(\"vectorstore.pkl\", \"wb\") as f:\n",
    "            pickle.dump(vectorstore, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "111\n",
      "222\n",
      "333\n"
     ]
    }
   ],
   "source": [
    "embed_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if vectorstore.pkl exists\n",
    "if  os.path.exists(\"vectorstore.pkl\"):\n",
    "    with open(\"vectorstore.pkl\", 'rb') as f:\n",
    "        docsearch = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='History' lookup_str='' metadata={'source': 'data/nakheel.txt'} lookup_index=0\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter your query: \")\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ChatVectorDBChain\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "template = \"\"\"You are an AI assistant for answering questions about the Document you have uploaded.\n",
    "You are given the following extracted parts of a long document and a question. Provide a conversational answer. at the end of your answer, add a newline and return a python list of up to three wikipedia topics which are related to the context and question leading with a \"#\" like this wihout mentioning anything else:\n",
    "#['topic1', 'topic2', 'topic3']\n",
    "\n",
    "If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
    "\n",
    "Question: {question}\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "Answer in Markdown:\"\"\"\n",
    "QA_PROMPT = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    "\n",
    "\n",
    "def get_chain(vectorstore):\n",
    "    llm = OpenAI(temperature=0) #gpt-3.5-turbo\n",
    "    qa_chain = ChatVectorDBChain.from_llm(\n",
    "        llm,\n",
    "        vectorstore,\n",
    "        qa_prompt=QA_PROMPT,\n",
    "        condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    \n",
    "    )\n",
    "    return qa_chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f98d6545a8c8823af2d31d4f3495c64f9ce8c884c13cd035e8ae9649e78e139e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
